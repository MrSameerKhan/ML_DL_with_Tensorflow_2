{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEVPBRPOCTmi"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwQ4hBniKi5J"
      },
      "source": [
        "### Articles to read:\n",
        "- [6 ways to perform tokenization](https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/)\n",
        "- [All about tokenization](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXoYcecxMXdK"
      },
      "source": [
        "We can use NLTK for tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y9RLRiiBuPQ"
      },
      "source": [
        "# sentence\n",
        "s = \"hi, how are you doing today?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvJA1hBvMxOz",
        "outputId": "7c853973-d463-48a3-f669-6af1c560a612"
      },
      "source": [
        "# split sentence by space\n",
        "s.split(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi,', 'how', 'are', 'you', 'doing', 'today?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wBclkxiM3L5",
        "outputId": "7752cfb8-9196-4366-e7cd-d9fb3c922a66"
      },
      "source": [
        "# avoiding space to be treated as word\n",
        "s.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi,', 'how', 'are', 'you', 'doing', 'today?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csT7YFnPNEBJ",
        "outputId": "3bdd63bd-d334-4f46-a7ac-9c2d9d55425b"
      },
      "source": [
        "import re\n",
        "# replace all kinds of punctuations \n",
        "re.sub(r\"[^\\w]\", \" \", s).split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'how', 'are', 'you', 'doing', 'today']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duimnr04NaEA"
      },
      "source": [
        "We can see that we only got words here replacing all the punctuations and space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZb_NFpuNjyq"
      },
      "source": [
        "### Tokenization using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKR13_7yNWk3",
        "outputId": "589787c4-d436-4496-8c1f-b162e38647b2"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLIes-gNy0h",
        "outputId": "0dd5fc03-c2a3-4ffc-cb6e-26daab1ee535"
      },
      "source": [
        "\n",
        "word_tokenize(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', ',', 'how', 'are', 'you', 'doing', 'today', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZu-IVccN1af",
        "outputId": "95b323a1-0567-4ad6-df9a-3d7bf64723e2"
      },
      "source": [
        "wordpunct_tokenize(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', ',', 'how', 'are', 'you', 'doing', 'today', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFFucWVlN_rZ",
        "outputId": "f7bbe2d8-2143-4838-93d5-76def61f8781"
      },
      "source": [
        "# tokenizing by sentence\n",
        "s = \"hi, how are you doing today? I am fine\"\n",
        "sent_tokenize(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi, how are you doing today?', 'I am fine']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfDwB_xIS-mU"
      },
      "source": [
        "### Tokenization using Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5uWkDFpTo1U"
      },
      "source": [
        "#### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1OuqEjLObiQ",
        "outputId": "ffddee37-a432-4b71-b482-8f1e449e5499"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "# nlp object is used to create documents with linguistic annotations\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "token_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded',\n",
              " 'in',\n",
              " '2002',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’s',\n",
              " 'mission',\n",
              " 'is',\n",
              " 'to',\n",
              " 'enable',\n",
              " 'humans',\n",
              " 'to',\n",
              " 'become',\n",
              " 'a',\n",
              " 'spacefaring',\n",
              " 'civilization',\n",
              " 'and',\n",
              " 'a',\n",
              " 'multi',\n",
              " '-',\n",
              " 'planet',\n",
              " '\\n',\n",
              " 'species',\n",
              " 'by',\n",
              " 'building',\n",
              " 'a',\n",
              " 'self',\n",
              " '-',\n",
              " 'sustaining',\n",
              " 'city',\n",
              " 'on',\n",
              " 'Mars',\n",
              " '.',\n",
              " 'In',\n",
              " '2008',\n",
              " ',',\n",
              " 'SpaceX',\n",
              " '’s',\n",
              " 'Falcon',\n",
              " '1',\n",
              " 'became',\n",
              " 'the',\n",
              " 'first',\n",
              " 'privately',\n",
              " 'developed',\n",
              " '\\n',\n",
              " 'liquid',\n",
              " '-',\n",
              " 'fuel',\n",
              " 'launch',\n",
              " 'vehicle',\n",
              " 'to',\n",
              " 'orbit',\n",
              " 'the',\n",
              " 'Earth',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hHvbZLVTshr"
      },
      "source": [
        "#### Sentence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD1TrG-lTeSU",
        "outputId": "8f88ea6c-d5a4-4926-a685-191cbabc5826"
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "# create the pipeline 'sentencizer' component\n",
        "sbd = nlp.create_pipe('sentencizer')\n",
        "\n",
        "# add the component to the pipeline\n",
        "nlp.add_pipe(sbd)\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "# nlp object is used to create documents with linguistic annotations\n",
        "doc = nlp(text)\n",
        "\n",
        "# create list of sentence tokens\n",
        "sents_list = []\n",
        "for sent in doc.sents:\n",
        "    sents_list.append(sent)\n",
        "print(sents_list)\n",
        "print(len(sents_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
            "species by building a self-sustaining city on Mars., In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
            "liquid-fuel launch vehicle to orbit the Earth.]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql09vK71knyc"
      },
      "source": [
        "Will be adding more as I find new techniques ...."
      ]
    }
  ]
}