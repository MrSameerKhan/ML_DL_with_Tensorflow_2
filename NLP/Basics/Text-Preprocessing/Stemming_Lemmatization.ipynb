{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stemming_Lemmatization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGZl5OlRy2SE"
      },
      "source": [
        "# Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNAlKueAxEFF",
        "outputId": "4cc36166-0354-4ec1-d3ae-cdc637d46f96"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fkcYqQf-Fui"
      },
      "source": [
        "## Lancaster Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REaElwfo-UMg"
      },
      "source": [
        "This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. It’s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8l-YZ-R-VzM"
      },
      "source": [
        "from nltk.stem import LancasterStemmer"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eys8_nH0-d-m"
      },
      "source": [
        "lancaster = LancasterStemmer()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F7kzKTX-d7m",
        "outputId": "90c342c1-90d8-44ff-f96e-6246162c7994"
      },
      "source": [
        "print(lancaster.stem(\"cats\"))\n",
        "print(lancaster.stem(\"trouble\"))\n",
        "print(lancaster.stem(\"troubling\"))\n",
        "print(lancaster.stem(\"troubled\"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExUYgA6O-0VP"
      },
      "source": [
        "We can see that LancasterStemmer stems the words to not so meaningful words (example : troubl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VITnhWR2_UWA"
      },
      "source": [
        "## PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j20KOpen_ZR_"
      },
      "source": [
        "This stemming algorithm is an older one. It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. It’s not too complex and development on it is frozen. Typically, it’s a nice starting basic stemmer, but it’s not really advised to use it for any production/complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr6u1g3j-d4S"
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vpBT1aW-d1R"
      },
      "source": [
        "porter = PorterStemmer()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8BezR4w-dyl",
        "outputId": "220d448e-e343-4a38-ae44-c249cb11c3dd"
      },
      "source": [
        "print(porter.stem(\"troubled\"))\n",
        "print(porter.stem(\"troubling\"))\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"fishing\"))\n",
        "print(porter.stem(\"fished\"))\n",
        "print(porter.stem(\"fishes\"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "troubl\n",
            "troubl\n",
            "cat\n",
            "fish\n",
            "fish\n",
            "fish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDjELyzh_4UX"
      },
      "source": [
        "## Snowball Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP8hMs_J_8IR"
      },
      "source": [
        "This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2NZFqk3z_Og",
        "outputId": "c6d3e160-8547-421f-8395-ebb01ca1d6ef"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language=\"english\")\n",
        "# languages supported by SnowballStemmer\n",
        "SnowballStemmer.languages"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipK76osX0BOl"
      },
      "source": [
        "stemmer1 = SnowballStemmer(\"english\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mn_wyAUo0JIV",
        "outputId": "37fa914f-9590-4d72-d6b2-beccfa0e1442"
      },
      "source": [
        "stemmer1.stem(\"fishing\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fish'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1CbbjS8H0K_Z",
        "outputId": "d23be796-ae72-47ce-891a-c218db777ca9"
      },
      "source": [
        "stemmer1.stem(\"fishes\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fish'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA95STEc0NX5"
      },
      "source": [
        "stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GNPkPzwz0eRE",
        "outputId": "a02eabbe-74e2-4b12-d4e8-2617ac46951f"
      },
      "source": [
        "# not ignored stopword\n",
        "stemmer2.stem(\"having\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'having'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VHc_QRkP1IUl",
        "outputId": "cf607507-c691-4cc8-8055-51d30b393f28"
      },
      "source": [
        "# ignored stopword\n",
        "stemmer1.stem(\"having\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkvf8wBOAv_2"
      },
      "source": [
        "### Stemming on a paragraph\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHHK2NilA0Jm"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUvorw9-By2K"
      },
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
        "               the world have come and invaded us, captured our lands, conquered our minds. \n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
        "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
        "               We have not grabbed their land, their culture, \n",
        "               their history and tried to enforce our way of life on them. \n",
        "               Why? Because we respect the freedom of others.That is why my \n",
        "               first vision is that of freedom. I believe that India got its first vision of \n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
        "               I see four milestones in my career\"\"\""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peOEG1YfByve"
      },
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayEHiJ2eCOVG",
        "outputId": "8d4aa381-0448-4204-fc02-96198daa4432"
      },
      "source": [
        "# all stopwords in english (supported by nltk)\n",
        "stopwords.words('english')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_pXwZp0Byq-",
        "outputId": "9d59becd-82b4-44b9-ee0e-e4d56624e703"
      },
      "source": [
        "stemmer = SnowballStemmer(language=\"english\")\n",
        "\n",
        "# stemming\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "    print(sentences[i])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i three vision india .\n",
            "in 3000 year histori , peopl world come invad u , captur land , conquer mind .\n",
            "from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot u , took .\n",
            "yet done nation .\n",
            "we conquer anyon .\n",
            "we grab land , cultur , histori tri enforc way life .\n",
            "whi ?\n",
            "becaus respect freedom others.that first vision freedom .\n",
            "i believ india got first vision 1857 , start war independ .\n",
            "it freedom must protect nurtur build .\n",
            "if free , one respect u .\n",
            "my second vision india ’ develop .\n",
            "for fifti year develop nation .\n",
            "it time see develop nation .\n",
            "we among top 5 nation world term gdp .\n",
            "we 10 percent growth rate area .\n",
            "our poverti level fall .\n",
            "our achiev global recognis today .\n",
            "yet lack self-confid see develop nation , self-reli self-assur .\n",
            "isn ’ incorrect ?\n",
            "i third vision .\n",
            "india must stand world .\n",
            "becaus i believ unless india stand world , one respect u .\n",
            "onli strength respect strength .\n",
            "we must strong militari power also econom power .\n",
            "both must go hand-in-hand .\n",
            "my good fortun work three great mind .\n",
            "dr. vikram sarabhai dept .\n",
            "space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .\n",
            "i lucki work three close consid great opportun life .\n",
            "i see four mileston career\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N3N_Oot1YEr"
      },
      "source": [
        "# Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKzcMwQd1MqJ"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7y-m4ZLr1wEq",
        "outputId": "6c6effd9-9e58-4a5e-b226-277945585711"
      },
      "source": [
        "lemmatizer.lemmatize(\"fishing\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fishing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LzjBKMp02A6R",
        "outputId": "a667c702-d5ac-4810-c12c-aea26736f530"
      },
      "source": [
        "lemmatizer.lemmatize(\"corpora\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'corpus'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTFuKg_kGto1"
      },
      "source": [
        "### WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcvdhAAW2MOn",
        "outputId": "5a7393f8-1c22-481f-cc70-c58c8a90a10c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAvNeiuoG2kn"
      },
      "source": [
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgvmQUcvG4ep",
        "outputId": "99ba4ec5-13fc-445c-ec37-6408de993854"
      },
      "source": [
        "# single word lemmatization examples \n",
        "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',  \n",
        "         'driving', 'died', 'tried', 'feet'] \n",
        "for words in list1: \n",
        "    print(words + \" ---> \" + wnl.lemmatize(words)) "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kites ---> kite\n",
            "babies ---> baby\n",
            "dogs ---> dog\n",
            "flying ---> flying\n",
            "smiling ---> smiling\n",
            "driving ---> driving\n",
            "died ---> died\n",
            "tried ---> tried\n",
            "feet ---> foot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PGKw1b-G7wp",
        "outputId": "6538d1ae-eed1-4870-ba86-da5731d2febe"
      },
      "source": [
        "# sentence lemmatization examples \n",
        "string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n",
        "  \n",
        "# Converting String into tokens \n",
        "list2 = nltk.word_tokenize(string) \n",
        "print(list2) \n",
        "  \n",
        "lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2]) \n",
        "  \n",
        "print(lemmatized_string)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on', 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
            "the cat is sitting with the bat on the striped mat under many flying goose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmw922FZICRe"
      },
      "source": [
        "### WordNet Lemmatizer (with POS tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4oqmd5fIKBI"
      },
      "source": [
        "In the above approach, we observed that Wordnet results were not up to the mark. Words like ‘sitting’, ‘flying’ etc remained the same after lemmatization. This is because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of Speech) tags. \n",
        "\n",
        "We add a tag with a particular word defining its type (verb, noun, adjective etc). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaXfr14XH3uH",
        "outputId": "97d2d19f-0d00-4c1c-e081-6f7c5ed0f7ca"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3IU5PDWIYAv"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skx-_kSTIbpj"
      },
      "source": [
        "# define function to lemmatize each word with its POS tag\n",
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-MKpIHsIfGy",
        "outputId": "957037b8-9fe0-42ae-cd33-3a8fa37deb89"
      },
      "source": [
        "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
        "  \n",
        "# tokenize the sentence and find the POS tag for each token \n",
        "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))   \n",
        "  \n",
        "print(pos_tagged) "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('badly', 'RB'), ('flying', 'VBG'), ('geese', 'JJ')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5LfLFIfJENI",
        "outputId": "4be5972e-9cf5-4a02-fa9f-432720deb61e"
      },
      "source": [
        "# As you may have noticed, the above pos tags are a little confusing. \n",
        "  \n",
        "# we use our own pos_tagger function to make things simpler to understand. \n",
        "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged)) \n",
        "print(wordnet_tagged) \n",
        "\n",
        "lemmatized_sentence = [] \n",
        "for word, tag in wordnet_tagged: \n",
        "    if tag is None: \n",
        "        # if there is no available tag, append the token as is \n",
        "        lemmatized_sentence.append(word) \n",
        "    else:         \n",
        "        # else use the tag to lemmatize the token \n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
        "lemmatized_sentence = \" \".join(lemmatized_sentence) \n",
        "  \n",
        "print(lemmatized_sentence) "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), ('mat', 'n'), ('under', None), ('many', 'a'), ('badly', 'r'), ('flying', 'v'), ('geese', 'a')]\n",
            "the cat be sit with the bat on the striped mat under many badly fly geese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXx_BdltZn1s"
      },
      "source": [
        "#### Will be adding more techniques as I discover ....\n"
      ]
    }
  ]
}