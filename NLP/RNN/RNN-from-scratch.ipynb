{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## RNN from scratch in PyTorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](https://miro.medium.com/max/840/1*o65pRKyHxhw7m8LgMbVERg.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, let’s build the computation graph for a single-layer RNN. Again, we are not concerned with the math for now, I just want to show you the PyTorch operations needed to build your RNN models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleRNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        super(SingleRNN, self).__init__()\n",
    "\n",
    "        self.Wx = torch.randn(n_inputs, n_neurons)  # 4 x 1\n",
    "        self.Wy = torch.randn(n_neurons, n_neurons) # 1 x 1\n",
    "\n",
    "        self.b = torch.zeros(1, n_neurons) # 1 x 4\n",
    "    \n",
    "    def forward(self, X0, X1):\n",
    "        self.Y0 = torch.tanh(torch.mm(X0, self.Wx) + self.b) # 4 x 1\n",
    "\n",
    "        self.Y1 = torch.tanh(torch.mm(self.Y0, self.Wy) + \n",
    "                             torch.mm(X1, self.Wx) + self.b) # 4 x 1\n",
    "        \n",
    "        return self.Y0, self.Y1"
   ]
  },
  {
   "source": [
    "The forward function computes two outputs — one for each time step (two overall). Note that we are using tanh as the non-linearity (activation function) via torch.tanh(...)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is how the data is fed into RNN:\n",
    "\n",
    "![](https://miro.medium.com/max/764/1*xCj9h3f2kekfqN_dMCpcag.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUT = 4\n",
    "N_NEURONS = 1\n",
    "\n",
    "\n",
    "X0_batch = torch.tensor([[0,1,2,0], [3,4,5,0], \n",
    "                         [6,7,8,0], [9,0,1,0]],\n",
    "                        dtype = torch.float) #t=0 => 4 X 4\n",
    "\n",
    "X1_batch = torch.tensor([[9,8,7,0], [0,0,0,0], \n",
    "                         [6,5,4,0], [3,2,1,0]],\n",
    "                        dtype = torch.float) #t=1 => 4 X 4\n",
    "model = SingleRNN(N_INPUT, N_NEURONS)\n",
    "\n",
    "Y0_val, Y1_val = model(X0_batch, X1_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.9905],\n        [-0.9425],\n        [-0.6910],\n        [ 1.0000]])\ntensor([[ 1.0000],\n        [-0.0332],\n        [ 0.9997],\n        [ 0.9986]])\n"
     ]
    }
   ],
   "source": [
    "print(Y0_val)\n",
    "print(Y1_val)"
   ]
  },
  {
   "source": [
    "### Increasing neurons in RNN Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![](https://miro.medium.com/max/840/1*KLBXIeszx_cqkYs3-EXHwg.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        super(BasicRNN, self).__init__()\n",
    "\n",
    "        self.Wx = torch.randn(n_inputs, n_neurons) # n_inputs x n_neurons\n",
    "        self.Wy = torch.randn(n_neurons, n_neurons) # n_neurons x n_neurons\n",
    "\n",
    "        self.b = torch.zeros(1, n_neurons) # 1 x neurons\n",
    "\n",
    "    def forward(self, X0, X1):\n",
    "        self.Y0 = torch.tanh(torch.mm(X0, self.Wx) + self.b) # batch_size x n_neurons\n",
    "        self.Y1 = torch.tanh(torch.mm(self.Y0, self.Wy) + torch.mm(X1, self.Wx) + self.b) # batch_size x n_neurons\n",
    "\n",
    "        return self.Y0, self.Y1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUT = 3 # number of features in input\n",
    "N_NEURONS = 5 # number of units in layer\n",
    "\n",
    "X0_batch = torch.tensor([[0,1,2], [3,4,5], \n",
    "                         [6,7,8], [9,0,1]],\n",
    "                        dtype = torch.float) #t=0 => 4 X 3\n",
    "\n",
    "X1_batch = torch.tensor([[9,8,7], [0,0,0], \n",
    "                         [6,5,4], [3,2,1]],\n",
    "                        dtype = torch.float) #t=1 => 4 X 3\n",
    "\n",
    "model = BasicRNN(N_INPUT, N_NEURONS)\n",
    "\n",
    "Y0_val, Y1_val = model(X0_batch, X1_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.9998, -0.8551, -0.9390,  0.7864,  0.9697],\n        [-1.0000, -0.9927, -1.0000, -0.6045,  1.0000],\n        [-1.0000, -0.9997, -1.0000, -0.9856,  1.0000],\n        [-0.9996,  0.9989, -1.0000, -1.0000,  1.0000]])\ntensor([[-1.0000, -0.9955, -1.0000, -0.9974,  1.0000],\n        [-0.9169,  0.7480,  0.9892,  0.9962, -0.8102],\n        [-1.0000, -0.5568, -1.0000, -0.8928,  1.0000],\n        [-1.0000,  0.9568, -0.9894,  0.1957,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(Y0_val)\n",
    "print(Y1_val)"
   ]
  }
 ]
}